#!/usr/bin/env python3
"""
Inspect Base Model Checkpoint

Quick script to see what's inside checkpoints/base/base_model_last.ckpt
"""

import torch
from pathlib import Path

def inspect_checkpoint(checkpoint_path="checkpoints/base/base_model_last.ckpt"):
    """Inspect a PyTorch Lightning checkpoint."""
    
    print("ðŸ” Inspecting Checkpoint")
    print("=" * 70)
    
    # Check if file exists
    ckpt_file = Path(checkpoint_path)
    if not ckpt_file.exists():
        print(f"âŒ Checkpoint not found: {checkpoint_path}")
        return
    
    # Get file size
    file_size = ckpt_file.stat().st_size / (1024 * 1024)  # MB
    print(f"ðŸ“ File: {checkpoint_path}")
    print(f"ðŸ“Š Size: {file_size:.2f} MB")
    
    # Load checkpoint
    print("\nðŸ“¥ Loading checkpoint...")
    try:
        ckpt = torch.load(checkpoint_path, map_location='cpu')
        print("âœ… Checkpoint loaded successfully")
    except Exception as e:
        print(f"âŒ Failed to load: {e}")
        return
    
    print("\n" + "=" * 70)
    print("ðŸ“‹ Checkpoint Contents")
    print("=" * 70)
    
    # Top-level keys
    print(f"\nðŸ”‘ Top-level keys:")
    for key in ckpt.keys():
        print(f"  â€¢ {key}")
    
    # Epoch information
    if 'epoch' in ckpt:
        print(f"\nâ±ï¸  Training Progress:")
        print(f"  â€¢ Epoch: {ckpt['epoch']}")
        if 'global_step' in ckpt:
            print(f"  â€¢ Global Step: {ckpt['global_step']}")
    
    # PyTorch Lightning version
    if 'pytorch-lightning_version' in ckpt:
        print(f"\nâš¡ PyTorch Lightning Version: {ckpt['pytorch-lightning_version']}")
    
    # Model state
    if 'state_dict' in ckpt:
        state_dict = ckpt['state_dict']
        print(f"\nðŸ§  Model State:")
        print(f"  â€¢ Total parameters: {len(state_dict)} tensors")
        
        # Calculate total parameters
        total_params = sum(p.numel() for p in state_dict.values())
        print(f"  â€¢ Total parameter count: {total_params:,}")
        print(f"  â€¢ Estimated model size: {total_params * 4 / (1024**2):.2f} MB (FP32)")
        
        # Sample parameter names
        param_names = list(state_dict.keys())
        print(f"\n  ðŸ“ Sample parameter names (first 10):")
        for name in param_names[:10]:
            shape = tuple(state_dict[name].shape)
            print(f"     â€¢ {name}: {shape}")
        
        if len(param_names) > 10:
            print(f"     ... and {len(param_names) - 10} more")
    
    # Optimizer state
    if 'optimizer_states' in ckpt:
        opt_states = ckpt['optimizer_states']
        print(f"\nðŸŽ¯ Optimizer State:")
        print(f"  â€¢ Number of optimizers: {len(opt_states)}")
        if opt_states:
            print(f"  â€¢ Optimizer type: {opt_states[0].get('optimizer_type', 'Unknown')}")
    
    # Learning rate schedulers
    if 'lr_schedulers' in ckpt:
        lr_schedulers = ckpt['lr_schedulers']
        print(f"\nðŸ“ˆ LR Schedulers:")
        print(f"  â€¢ Number of schedulers: {len(lr_schedulers)}")
    
    # Callbacks
    if 'callbacks' in ckpt:
        callbacks = ckpt['callbacks']
        print(f"\nðŸ”” Callbacks:")
        for callback_name, callback_state in callbacks.items():
            print(f"  â€¢ {callback_name}")
    
    # Hyperparameters (if available)
    if 'hyper_parameters' in ckpt:
        print(f"\nâš™ï¸  Hyperparameters:")
        hparams = ckpt['hyper_parameters']
        for key, value in list(hparams.items())[:10]:
            print(f"  â€¢ {key}: {value}")
        if len(hparams) > 10:
            print(f"  ... and {len(hparams) - 10} more")
    
    print("\n" + "=" * 70)
    print("âœ… Inspection Complete")
    print("=" * 70)
    
    # Resume instructions
    print("\nðŸ’¡ To resume training from this checkpoint:")
    print("\n1ï¸âƒ£  Simplest way (PyTorch Lightning):")
    print("   trainer.fit(model, datamodule, ckpt_path='checkpoints/base/base_model_last.ckpt')")
    
    print("\n2ï¸âƒ£  With checkpoint management:")
    print("   python resume_base_training.py")
    
    print("\n3ï¸âƒ£  Manual loading:")
    print("   ckpt = torch.load('checkpoints/base/base_model_last.ckpt')")
    print("   model.load_state_dict(ckpt['state_dict'])")
    
    return ckpt


if __name__ == "__main__":
    import sys
    
    # Allow custom checkpoint path
    if len(sys.argv) > 1:
        checkpoint_path = sys.argv[1]
    else:
        checkpoint_path = "checkpoints/base/base_model_last.ckpt"
    
    inspect_checkpoint(checkpoint_path)
