# Base Training Configuration for DiffiT
# With checkpoint management for resumable training

training:
  # Model configuration
  model_config: "configs/models/unet_config.yaml"
  
  # Dataset configuration
  dataset: "CIFAR"  # Options: "CIFAR", "CIFAR100", "IMAGENETTE"
  
  # Training hyperparameters
  num_epochs: 20
  batch_size_train: 64
  batch_size_test: 16
  learning_rate: 0.001
  weight_decay: 0.0
  
  # Optimization
  optimizer: "Adam"
  scheduler: null
  gradient_clip_val: 1.0
  
  # PyTorch Lightning trainer settings
  accelerator: "auto"
  devices: "auto"
  precision: "32-true"
  
  # Monitoring and checkpointing
  monitor_metric: "train_loss"
  save_top_k: 3
  check_val_every_n_epoch: 1
  log_every_n_steps: 25
  
  # Paths
  output_dir: "./checkpoints/base/"
  experiment_name: "diffit_base_training"
  
  # Checkpoint Management for Base Model Training
  checkpoint:
    enabled: true
    base_dir: "checkpoints/base_training"
    run_number: null  # Auto-increment if null
    save_every_n_epochs: 1
    keep_last_n: 3  # Keep last 3 checkpoints
    monitor: "train_loss"
    mode: "min"
    resume_from_run: null  # Run number to resume from (null for new run)
    resume_checkpoint: null  # Specific checkpoint path to resume from
  
# Data preprocessing
data:
  img_size: 32
  num_workers: 2
  pin_memory: true
  
  # Data augmentation
  augmentation:
    enabled: false
    horizontal_flip: 0.5
    rotation_degrees: 5
