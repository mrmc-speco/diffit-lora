{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Resume Base Model Training from Checkpoint\n",
        "\n",
        "This notebook helps you resume training from an existing base model checkpoint with configurable parameters.\n",
        "\n",
        "## Features\n",
        "- Resume from existing checkpoint\n",
        "- Configurable training parameters\n",
        "- Configurable input/output directories\n",
        "- Automatic checkpoint compatibility fixing\n",
        "- Progress monitoring\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration Parameters\n",
        "\n",
        "Customize all parameters below before starting training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURABLE PARAMETERS\n",
        "# ============================================================================\n",
        "\n",
        "# --- Input/Output Directories ---\n",
        "CHECKPOINT_TO_RESUME = \"checkpoints/base/base_model_last.ckpt\"  # Path to existing checkpoint\n",
        "OUTPUT_DIR = \"checkpoints/base_training\"  # Where to save new checkpoints\n",
        "CONFIG_DIR = \"configs\"  # Directory containing config files\n",
        "\n",
        "# --- Training Configuration ---\n",
        "NUM_EPOCHS = 3  # Number of epochs to train\n",
        "LEARNING_RATE = 0.001  # Learning rate\n",
        "BATCH_SIZE = 128  # Batch size (if overriding config)\n",
        "\n",
        "# --- Dataset Configuration ---\n",
        "DATASET = \"CIFAR\"  # Options: 'CIFAR', 'CIFAR100', 'CIFAR100_TREES', 'CIFAR100_ANIMALS', 'IMAGENETTE'\n",
        "\n",
        "# --- Model Architecture ---\n",
        "D_MODEL = 128  # Model dimension\n",
        "NUM_HEADS = 2  # Number of attention heads\n",
        "DROPOUT = 0.1  # Dropout rate\n",
        "D_FF = 256  # Feed-forward dimension\n",
        "IMG_SIZE = 32  # Input image size\n",
        "DENOISING_STEPS = 500  # Number of denoising steps\n",
        "\n",
        "# --- Training Settings ---\n",
        "EXPERIMENT_NAME = \"diffit_base_resumed\"  # Name for this training run\n",
        "SAVE_EVERY_N_EPOCHS = 1  # Save checkpoint every N epochs\n",
        "KEEP_LAST_N_CHECKPOINTS = 3  # Keep only last N checkpoints\n",
        "GRADIENT_CLIP_VAL = 1.0  # Gradient clipping value\n",
        "LOG_EVERY_N_STEPS = 25  # Log metrics every N steps\n",
        "\n",
        "# --- Hardware Settings ---\n",
        "ACCELERATOR = \"auto\"  # Options: 'auto', 'gpu', 'cpu', 'tpu'\n",
        "DEVICES = \"auto\"  # Options: 'auto', 1, 2, [0,1], etc.\n",
        "PRECISION = \"32-true\"  # Options: '32-true', '16-mixed', 'bf16-mixed'\n",
        "\n",
        "# --- Advanced Options ---\n",
        "VERBOSE = True  # Print detailed progress information\n",
        "FIX_CHECKPOINT_COMPATIBILITY = True  # Automatically fix checkpoint compatibility issues\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor\n",
        "\n",
        "# Add project to path\n",
        "project_root = Path.cwd()\n",
        "if str(project_root) not in sys.path:\n",
        "    sys.path.insert(0, str(project_root))\n",
        "\n",
        "from diffit.training.base_checkpoint_callbacks import (\n",
        "    BaseModelCheckpointCallback,\n",
        "    ResumeBaseModelCallback\n",
        ")\n",
        "from diffit.training.data import DiffiTDataModule\n",
        "from diffit.models.unet import UShapedNetwork\n",
        "\n",
        "print(\"‚úÖ Imports successful!\")\n",
        "print(f\"üìÅ Project root: {project_root}\")\n",
        "print(f\"üîß PyTorch Lightning version: {pl.__version__}\")\n",
        "print(f\"üîß PyTorch version: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Verify Checkpoint and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîç Verifying checkpoint and configuration...\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check checkpoint exists\n",
        "checkpoint_path = Path(CHECKPOINT_TO_RESUME)\n",
        "if checkpoint_path.exists():\n",
        "    file_size_mb = checkpoint_path.stat().st_size / (1024 * 1024)\n",
        "    print(f\"‚úÖ Checkpoint found: {CHECKPOINT_TO_RESUME}\")\n",
        "    print(f\"   Size: {file_size_mb:.2f} MB\")\n",
        "else:\n",
        "    print(f\"‚ùå ERROR: Checkpoint not found at {CHECKPOINT_TO_RESUME}\")\n",
        "    print(\"   Please update CHECKPOINT_TO_RESUME variable\")\n",
        "    raise FileNotFoundError(f\"Checkpoint not found: {CHECKPOINT_TO_RESUME}\")\n",
        "\n",
        "# Determine dataset config file\n",
        "dataset_config_map = {\n",
        "    'CIFAR': 'cifar10.yaml',\n",
        "    'CIFAR100': 'cifar100.yaml',\n",
        "    'CIFAR100_TREES': 'cifar100_trees_only.yaml',\n",
        "    'CIFAR100_ANIMALS': 'cifar100_animals_only.yaml',\n",
        "    'CIFAR100_VEHICLES': 'cifar100_vehicles_only.yaml',\n",
        "    'IMAGENETTE': 'imagenette.yaml'\n",
        "}\n",
        "\n",
        "data_config_file = dataset_config_map.get(DATASET, f\"{DATASET.lower()}.yaml\")\n",
        "data_config_path = Path(CONFIG_DIR) / \"data\" / data_config_file\n",
        "\n",
        "if data_config_path.exists():\n",
        "    print(f\"\\n‚úÖ Dataset config found: {data_config_path}\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå ERROR: Dataset config not found at {data_config_path}\")\n",
        "    raise FileNotFoundError(f\"Dataset config not found: {data_config_path}\")\n",
        "\n",
        "# Check training config\n",
        "training_config_path = Path(CONFIG_DIR) / \"training\" / \"base_training.yaml\"\n",
        "if training_config_path.exists():\n",
        "    print(f\"‚úÖ Training config found: {training_config_path}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Warning: Training config not found at {training_config_path}\")\n",
        "    print(\"   Will use notebook parameters only\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Configuration Files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìã Loading configuration files...\\n\")\n",
        "\n",
        "# Load training config (if exists)\n",
        "training_config = {}\n",
        "if training_config_path.exists():\n",
        "    with open(training_config_path, 'r') as f:\n",
        "        config_data = yaml.safe_load(f)\n",
        "        training_config = config_data.get('training', {})\n",
        "    print(f\"‚úÖ Loaded training config from {training_config_path}\")\n",
        "\n",
        "# Override with notebook parameters\n",
        "training_config.update({\n",
        "    'num_epochs': NUM_EPOCHS,\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'dataset': DATASET,\n",
        "    'accelerator': ACCELERATOR,\n",
        "    'devices': DEVICES,\n",
        "    'precision': PRECISION,\n",
        "    'gradient_clip_val': GRADIENT_CLIP_VAL,\n",
        "    'log_every_n_steps': LOG_EVERY_N_STEPS\n",
        "})\n",
        "\n",
        "# Load data config\n",
        "with open(data_config_path, 'r') as f:\n",
        "    data_config = yaml.safe_load(f)\n",
        "\n",
        "print(f\"‚úÖ Loaded dataset config from {data_config_path}\")\n",
        "\n",
        "# Display final configuration\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä Final Training Configuration:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Epochs:              {training_config['num_epochs']}\")\n",
        "print(f\"Learning Rate:       {training_config['learning_rate']}\")\n",
        "print(f\"Dataset:             {training_config['dataset']}\")\n",
        "print(f\"Accelerator:         {training_config['accelerator']}\")\n",
        "print(f\"Devices:             {training_config['devices']}\")\n",
        "print(f\"Precision:           {training_config['precision']}\")\n",
        "print(f\"Gradient Clip:       {training_config['gradient_clip_val']}\")\n",
        "print(f\"\\nModel Architecture:\")\n",
        "print(f\"D Model:             {D_MODEL}\")\n",
        "print(f\"Num Heads:           {NUM_HEADS}\")\n",
        "print(f\"Dropout:             {DROPOUT}\")\n",
        "print(f\"Feed Forward Dim:    {D_FF}\")\n",
        "print(f\"Image Size:          {IMG_SIZE}\")\n",
        "print(f\"Denoising Steps:     {DENOISING_STEPS}\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Setup Data Module\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üì¶ Setting up data module...\\n\")\n",
        "\n",
        "data_module = DiffiTDataModule(data_config)\n",
        "\n",
        "print(f\"‚úÖ Data module created for {DATASET}\")\n",
        "print(f\"   Config: {data_config_path.name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Initialize Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ü§ñ Initializing model...\\n\")\n",
        "\n",
        "# Determine device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"üñ•Ô∏è  Device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "\n",
        "# Initialize model\n",
        "model = UShapedNetwork(\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT,\n",
        "    d_ff=D_FF,\n",
        "    img_size=IMG_SIZE,\n",
        "    device=device,\n",
        "    denoising_steps=DENOISING_STEPS\n",
        ")\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\n‚úÖ Model initialized successfully!\")\n",
        "print(f\"   Total parameters:     {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Model size:           ~{total_params * 4 / 1024**2:.2f} MB (FP32)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Fix Checkpoint Compatibility (if needed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîß Checking checkpoint compatibility...\\n\")\n",
        "\n",
        "# Load checkpoint to inspect\n",
        "checkpoint = torch.load(CHECKPOINT_TO_RESUME, map_location='cpu')\n",
        "starting_epoch = checkpoint.get('epoch', 0)\n",
        "\n",
        "print(f\"üìä Checkpoint info:\")\n",
        "print(f\"   Current epoch: {starting_epoch}\")\n",
        "print(f\"   Will resume from epoch: {starting_epoch + 1}\")\n",
        "\n",
        "# Fix checkpoint compatibility if needed\n",
        "checkpoint_fixed = False\n",
        "checkpoint_to_use = CHECKPOINT_TO_RESUME\n",
        "\n",
        "if FIX_CHECKPOINT_COMPATIBILITY:\n",
        "    if 'pytorch-lightning_version' not in checkpoint:\n",
        "        print(f\"\\nüîß Adding PyTorch Lightning version: {pl.__version__}\")\n",
        "        checkpoint['pytorch-lightning_version'] = pl.__version__\n",
        "        checkpoint_fixed = True\n",
        "    \n",
        "    # Add empty optimizer and scheduler states if missing\n",
        "    if 'optimizer_states' not in checkpoint:\n",
        "        print(\"üîß Adding missing optimizer/scheduler states (weights-only checkpoint)\")\n",
        "        checkpoint['optimizer_states'] = []\n",
        "        checkpoint['lr_schedulers'] = []\n",
        "        checkpoint_fixed = True\n",
        "    \n",
        "    if checkpoint_fixed:\n",
        "        # Save fixed checkpoint to temporary location\n",
        "        temp_checkpoint = \"temp_fixed_checkpoint.ckpt\"\n",
        "        torch.save(checkpoint, temp_checkpoint)\n",
        "        checkpoint_to_use = temp_checkpoint\n",
        "        print(f\"‚úÖ Fixed checkpoint saved to: {temp_checkpoint}\")\n",
        "    else:\n",
        "        print(\"‚úÖ Checkpoint is compatible, no fixes needed\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Checkpoint compatibility fixing is disabled\")\n",
        "\n",
        "print(f\"\\nüì• Will resume from checkpoint: {checkpoint_to_use}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"‚öôÔ∏è  Setting up trainer and callbacks...\\n\")\n",
        "\n",
        "# Setup checkpoint callback\n",
        "checkpoint_callback = BaseModelCheckpointCallback(\n",
        "    base_dir=OUTPUT_DIR,\n",
        "    run_number=None,  # Will create new run\n",
        "    experiment_name=EXPERIMENT_NAME,\n",
        "    monitor='train_loss',\n",
        "    mode='min',\n",
        "    save_every_n_epochs=SAVE_EVERY_N_EPOCHS,\n",
        "    keep_last_n=KEEP_LAST_N_CHECKPOINTS,\n",
        "    verbose=VERBOSE\n",
        ")\n",
        "\n",
        "# Set starting epoch for checkpoint callback\n",
        "checkpoint_callback.set_starting_epoch(starting_epoch)\n",
        "\n",
        "# Learning rate monitor\n",
        "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "callbacks = [checkpoint_callback, lr_monitor]\n",
        "\n",
        "print(f\"‚úÖ Checkpoint callback configured:\")\n",
        "print(f\"   Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"   Experiment name: {EXPERIMENT_NAME}\")\n",
        "print(f\"   Save every: {SAVE_EVERY_N_EPOCHS} epoch(s)\")\n",
        "print(f\"   Keep last: {KEEP_LAST_N_CHECKPOINTS} checkpoint(s)\")\n",
        "\n",
        "# Setup trainer\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=NUM_EPOCHS,\n",
        "    callbacks=callbacks,\n",
        "    accelerator=ACCELERATOR,\n",
        "    devices=DEVICES,\n",
        "    precision=PRECISION,\n",
        "    gradient_clip_val=GRADIENT_CLIP_VAL,\n",
        "    log_every_n_steps=LOG_EVERY_N_STEPS,\n",
        "    default_root_dir=str(checkpoint_callback.logs_dir)\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Trainer configured successfully!\")\n",
        "print(f\"   Logs directory: {checkpoint_callback.logs_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Start Training\n",
        "\n",
        "‚ö†Ô∏è **Warning**: This will start the training process. Make sure all parameters are correctly configured above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üöÄ STARTING TRAINING\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Resuming from epoch {starting_epoch} ‚Üí Training until epoch {NUM_EPOCHS}\")\n",
        "print(f\"Checkpoint: {checkpoint_to_use}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\")\n",
        "print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "# Start training\n",
        "trainer.fit(model, datamodule=data_module, ckpt_path=checkpoint_to_use)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ TRAINING COMPLETED!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"üìÅ New checkpoints saved to: {checkpoint_callback.run_dir}\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Cleanup and Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Clean up temporary checkpoint if it was created\n",
        "if checkpoint_to_use == \"temp_fixed_checkpoint.ckpt\":\n",
        "    if os.path.exists(\"temp_fixed_checkpoint.ckpt\"):\n",
        "        os.remove(\"temp_fixed_checkpoint.ckpt\")\n",
        "        print(\"üßπ Cleaned up temporary checkpoint file\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä TRAINING SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Original checkpoint:  {CHECKPOINT_TO_RESUME}\")\n",
        "print(f\"Starting epoch:       {starting_epoch}\")\n",
        "print(f\"Final epoch:          {NUM_EPOCHS}\")\n",
        "print(f\"Epochs trained:       {NUM_EPOCHS - starting_epoch}\")\n",
        "print(f\"Output directory:     {checkpoint_callback.run_dir}\")\n",
        "print(f\"Logs directory:       {checkpoint_callback.logs_dir}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nüí° Next steps:\")\n",
        "print(\"1. Check the output directory for new checkpoints\")\n",
        "print(\"2. View training logs in TensorBoard (if installed)\")\n",
        "print(\"3. Use the best checkpoint for inference or fine-tuning\")\n",
        "print(\"\\n‚úÖ All done!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: Inspect Checkpoint Contents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Inspect the checkpoint to see what's inside\n",
        "print(\"üîç Checkpoint contents:\")\n",
        "print(\"=\"*70)\n",
        "for key in checkpoint.keys():\n",
        "    if key == 'state_dict':\n",
        "        print(f\"  {key}: {len(checkpoint[key])} model weights\")\n",
        "    elif isinstance(checkpoint[key], (list, dict)):\n",
        "        print(f\"  {key}: {type(checkpoint[key]).__name__} (len={len(checkpoint[key])})\")\n",
        "    else:\n",
        "        print(f\"  {key}: {checkpoint[key]}\")\n",
        "print(\"=\"*70)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
